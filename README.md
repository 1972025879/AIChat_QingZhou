# QuickStart for LLaMA

llamafactory-cli train examples/train_lora/llama3_lora_sft.yaml
llamafactory-cli chat examples/inference/llama3_lora_sft.yaml
llamafactory-cli export examples/merge_lora/llama3_lora_sft.yaml

---

### âœ… å‰æç¡®è®¤
- ä½ å·²æ¸…æ´—å‡º `other_to_you.jsonl`ï¼Œæ ¼å¼ä¸ºï¼š
  ```json
  {"input": "å¯¹æ–¹æ¶ˆæ¯", "output": "ä½ çš„å›å¤"}
  ```
- æœåŠ¡å™¨ï¼š2Ã—V100ï¼ˆ32GB/å¡ï¼Œå…±64GBï¼‰ï¼ŒLinuxï¼Œæ”¯æŒ `bf16`
- ç½‘ç»œï¼šå›½å†…ï¼Œä¼˜å…ˆä½¿ç”¨ **ModelScope**ï¼ˆè€Œé Hugging Faceï¼‰

---

## ğŸ§© é˜¶æ®µ 1ï¼šé€‰æ‹©åŸºç¡€æ¨¡å‹ï¼ˆBackboneï¼‰

âœ… **æ¨è**ï¼š`Qwen/Qwen1.5-7B-Chat`ï¼ˆä¸­æ–‡å¯¹è¯æœ€å¼ºï¼ŒModelScope å®˜æ–¹æ”¯æŒï¼‰

> ModelScope æ¨¡å‹ IDï¼š`qwen/Qwen1.5-7B-Chat`

---

## ğŸ›  é˜¶æ®µ 2ï¼šå‡†å¤‡ç¯å¢ƒ & æ•°æ®æ ¼å¼

### 1. å®‰è£… LLaMA-Factoryï¼ˆå¯ç”¨ ModelScope æ”¯æŒï¼‰
```bash
git clone https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory
pip install -e ".[torch,metrics]" --no-build-isolation
```

### 2. å¯ç”¨ ModelScope Hubï¼ˆå›½å†…åŠ é€Ÿï¼‰
```bash
export USE_MODELSCOPE_HUB=1
```

> æ­¤åæ‰€æœ‰ `model_name_or_path` å¯ç›´æ¥ä½¿ç”¨ ModelScope IDï¼Œå¦‚ `qwen/Qwen1.5-7B-Chat`

### 3. æ³¨å†Œä½ çš„ç§æœ‰æ•°æ®é›†

#### (1) å°†æ•°æ®æ”¾å…¥ç›®å½•ï¼ˆä¾‹å¦‚ï¼‰ï¼š
```bash
mkdir -p data/my_chat/
cp /your/path/other_to_you.jsonl data/my_chat/
```

#### (2) ç¼–è¾‘ `data/dataset_info.json`ï¼Œ**æ–°å¢æ¡ç›®**ï¼š
```json
{
  "my_chat": {
    "file_name": "my_chat/other_to_you.jsonl",
    "format": "alpaca"
  }
}
```

> âœ… æ ¼å¼è¯´æ˜ï¼š`alpaca` å¯¹åº” `{"input": "...", "output": "..."}`ï¼Œå®Œå…¨åŒ¹é…ä½ çš„æ•°æ®ã€‚

---

## ğŸ”¥ é˜¶æ®µ 3ï¼šLoRA å¾®è°ƒï¼ˆä½¿ç”¨ LLaMA-Factory å®˜æ–¹ CLIï¼‰

### 1. åˆ›å»ºè®­ç»ƒé…ç½®ï¼š`examples/train_lora/qwen1_5_7b_lora_sft.yaml`

```yaml
model_name_or_path: qwen/Qwen1.5-7B-Chat
template: qwen
finetuning_type: lora
lora_rank: 64
lora_alpha: 128
lora_dropout: 0.05
lora_target: all  # æˆ–æŒ‡å®š q_proj,v_proj ç­‰

dataset: my_chat
dataset_dir: data
split: train
max_samples: -1
overwrite_cache: true

per_device_train_batch_size: 4
gradient_accumulation_steps: 2
learning_rate: 3e-4
num_train_epochs: 3
lr_scheduler_type: cosine
warmup_ratio: 0.03

bf16: true
ddp_timeout: 18000
logging_steps: 10
save_steps: 500
output_dir: saves/qwen1_5_7b/lora/sft
```

### 2. å¯åŠ¨è®­ç»ƒï¼ˆè‡ªåŠ¨å¤šå¡ï¼Œæ— éœ€æ‰‹åŠ¨ torchrunï¼‰

```bash
CUDA_VISIBLE_DEVICES=0,1 llamafactory-cli train examples/train_lora/qwen1_5_7b_lora_sft.yaml
```

> âœ… LLaMA-Factory å†…éƒ¨è‡ªåŠ¨è°ƒç”¨ `torchrun`ï¼Œä½ åªéœ€æŒ‡å®š GPU å³å¯ã€‚  
> ğŸ’¡ æ˜¾å­˜å ç”¨ï¼š~28GB/å¡ï¼ˆV100 32GB è¶³å¤Ÿï¼‰

---

## ğŸš€ é˜¶æ®µ 4ï¼šåˆå¹¶ LoRA + æœ¬åœ° API éƒ¨ç½²

### 1. åˆå¹¶ LoRA åˆ°å®Œæ•´æ¨¡å‹

> âš ï¸ æ³¨æ„ï¼šåˆå¹¶æ—¶**ä¸èƒ½ä½¿ç”¨é‡åŒ–æ¨¡å‹**ï¼Œå¿…é¡»ç”¨åŸå§‹ BF16 æ¨¡å‹ã€‚

åˆ›å»º `examples/merge_lora/qwen1_5_7b_lora_sft.yaml`ï¼š
```yaml
model_name_or_path: qwen/Qwen1.5-7B-Chat
adapter_name_or_path: saves/qwen1_5_7b/lora/sft
template: qwen
export_dir: models/qwen1_5_7b_finetuned
export_size: 2  # åˆ†ç‰‡ä¿å­˜ï¼ˆå¯é€‰ï¼‰
export_device: cpu  # èŠ‚çœ GPU æ˜¾å­˜
```

æ‰§è¡Œåˆå¹¶ï¼š
```bash
llamafactory-cli export examples/merge_lora/qwen1_5_7b_lora_sft.yaml
```

è¾“å‡ºç›®å½•ï¼š`models/qwen1_5_7b_finetuned/`ï¼ˆå«å®Œæ•´ `pytorch_model.bin`ï¼‰

---

### 2. å¯åŠ¨ OpenAI å…¼å®¹ APIï¼ˆä½¿ç”¨ vLLMï¼Œé«˜æ€§èƒ½ï¼‰

```bash
# å®‰è£… vLLMï¼ˆå¦‚æœªå®‰è£…ï¼‰
pip install vllm

# å¯åŠ¨æœåŠ¡ï¼ˆè‡ªåŠ¨ä½¿ç”¨ 2 å¼  V100ï¼‰
python -m vllm.entrypoints.openai.api_server \
    --model ./models/qwen1_5_7b_finetuned \
    --tensor-parallel-size 2 \
    --port 8000 \
    --dtype bfloat16
```

> âœ… æ”¯æŒå¹¶å‘ã€æµå¼è¾“å‡ºã€OpenAI æ ‡å‡†æ¥å£

### 3. æµ‹è¯•è°ƒç”¨ï¼ˆPythonï¼‰

```python
from openai import OpenAI
client = OpenAI(base_url="http://localhost:8000/v1", api_key="none")
resp = client.chat.completions.create(
    model="qwen1.5-7b-finetuned",
    messages=[{"role": "user", "content": "ä½ è¿˜è®°å¾—æˆ‘ä»¬ç¬¬ä¸€æ¬¡è§é¢å—ï¼Ÿ"}],
    temperature=0.7,
    max_tokens=256
)
print(resp.choices[0].message.content)
```

---

## ğŸŒ é˜¶æ®µ 5ï¼ˆå¯é€‰æ‰©å±•ï¼‰ï¼šWeb ç•Œé¢ or RAG

### ğŸ”¹ å¿«é€Ÿ Web èŠå¤©ç•Œé¢ï¼ˆGradioï¼‰
```python
# web_demo.py
import gradio as gr
from openai import OpenAI

client = OpenAI(base_url="http://localhost:8000/v1", api_key="none")

def predict(message, history):
    messages = [{"role": "user", "content": msg} for msg, _ in history]
    messages.append({"role": "user", "content": message})
    resp = client.chat.completions.create(model="qwen1.5-7b-finetuned", messages=messages)
    return resp.choices[0].message.content

gr.ChatInterface(predict).launch(server_name="0.0.0.0", server_port=7860)
```

è¿è¡Œï¼š
```bash
python web_demo.py
```
è®¿é—®ï¼š`http://<your-server>:7860`

---

### ğŸ”¹ RAG å¢å¼ºï¼ˆæ£€ç´¢å†å²èŠå¤©ï¼‰
- å°†æœªç”¨äºè®­ç»ƒçš„èŠå¤©è®°å½•ï¼ˆå¦‚ `you_to_other.jsonl`ï¼‰å¯¼å…¥ **ChromaDB**
- åœ¨ API å‰åŠ æ£€ç´¢æ¨¡å—ï¼Œæ‹¼æ¥ä¸Šä¸‹æ–‡
- å¯ç”¨ LLaMA-Factory çš„ `rag` æ¨¡æ¿æˆ–è‡ªå®šä¹‰ pipeline

---

## âœ… æœ€ç»ˆæµç¨‹å›¾ï¼ˆæ›´æ–°ç‰ˆï¼‰

```
åŸå§‹å¾®ä¿¡JSON
     â†“
[æ•°æ®æ¸…æ´—] â†’ other_to_you.jsonlï¼ˆâœ… å·²å®Œæˆï¼‰
     â†“
[æ³¨å†Œæ•°æ®é›†] â†’ data/dataset_info.json + data/my_chat/
     â†“
[å¯ç”¨ModelScope] â†’ export USE_MODELSCOPE_HUB=1
     â†“
[LoRAå¾®è°ƒ] â†’ llamafactory-cli train ...ï¼ˆ2Ã—V100ï¼‰
     â†“
[åˆå¹¶æ¨¡å‹] â†’ llamafactory-cli export ...
     â†“
[éƒ¨ç½²API] â†’ vLLM OpenAI serverï¼ˆç«¯å£ 8000ï¼‰
     â†“
[ä½¿ç”¨] â†’ Python / curl / Web / RAG
```

---

## ğŸ“Œ å…³é”®æé†’

1. **ä¸è¦ç”¨ `torchrun` ç›´æ¥è°ƒè„šæœ¬**ï¼šLLaMA-Factory æ¨èä½¿ç”¨ `llamafactory-cli`ï¼Œå®ƒå·²å°è£… DDPã€DeepSpeedã€Ray ç­‰åç«¯ã€‚
2. **ModelScope ä¼˜å…ˆ**ï¼šå›½å†…ä¸‹è½½å¿«ï¼Œé¿å… HF ç½‘ç»œé—®é¢˜ã€‚
3. **LoRA åˆå¹¶å¿…é¡»ç”¨åŸå§‹æ¨¡å‹**ï¼šä¸èƒ½æ˜¯é‡åŒ–ç‰ˆï¼ˆå¦‚ GPTQ/AWQï¼‰ã€‚
4. **V100 ä¸æ”¯æŒ FlashAttention-2**ï¼šè®­ç»ƒæ—¶è¯·å…³é—­ `flash_attn`ï¼ˆé»˜è®¤å·²é€‚é…ï¼‰ã€‚

---

éœ€è¦æˆ‘ä¸ºä½ ç”Ÿæˆï¼š
- å®Œæ•´çš„ `qwen1_5_7b_lora_sft.yaml`ï¼Ÿ
- ä¸€é”®è®­ç»ƒ+åˆå¹¶+éƒ¨ç½²è„šæœ¬ï¼Ÿ
- Gradio å‰ç«¯ + RAG ç¤ºä¾‹ï¼Ÿ

éšæ—¶å‘Šè¯‰æˆ‘ï¼ä½ ç°åœ¨å¯ä»¥å®‰å…¨è¿›å…¥ **é˜¶æ®µ 2ï¼ˆç¯å¢ƒå‡†å¤‡ï¼‰** äº†ã€‚