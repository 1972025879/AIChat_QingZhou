å¥½çš„ï¼æ ¹æ®ä½ çš„ç¡¬ä»¶æ¡ä»¶ï¼ˆ**2Ã—V100ï¼Œå…± 60GB æ˜¾å­˜**ï¼‰ã€ç›®æ ‡ï¼ˆ**è®­ç»ƒä¸€ä¸ªèƒ½åƒä½ ä¸€æ ·å›å¤å¯¹æ–¹çš„ç§æœ‰èŠå¤©æœºå™¨äºº**ï¼‰ï¼Œä»¥åŠæ•°æ®è·¯å¾„ï¼š

```
/home/4T/wuhao_zjc/ab_new_document/AIChat_QingZhou/Dataset/json/wxid_wg6cp8uh982s22/other_to_you.jsonl
```

æˆ‘ä¸ºä½  **æ˜ç¡®æ¨èæ¨¡å‹ + ç†ç”± + ä¸‹ä¸€æ­¥è¡ŒåŠ¨æ¸…å•**ã€‚

---

### âœ… æœ€ç»ˆæ¨èæ¨¡å‹ï¼š**Qwen1.5-7B-Chat**

#### ğŸ” ä¸ºä»€ä¹ˆé€‰å®ƒï¼Ÿ
| ä¼˜åŠ¿ | è¯´æ˜ |
|------|------|
| **ä¸­æ–‡å¯¹è¯èƒ½åŠ›æå¼º** | é˜¿é‡Œé€šä¹‰åƒé—®ç³»åˆ—ï¼Œä¸“ä¸ºä¸­æ–‡ä¼˜åŒ–ï¼Œå¯¹æ—¥å¸¸èŠå¤©ã€æƒ…æ„Ÿè¡¨è¾¾ã€å£è¯­åŒ–å›å¤æ”¯æŒéå¸¸å¥½ |
| **å¼€æºå…è´¹å•†ç”¨** | Apache 2.0 åè®®ï¼Œå¯å®‰å…¨ç”¨äºæœ¬åœ°ç§æœ‰éƒ¨ç½² |
| **é€‚é… LLaMA-Factory** | ä¸€é”®æ”¯æŒ LoRA å¾®è°ƒã€æ¨ç†ã€å¯¼å‡º |
| **æ˜¾å­˜å‹å¥½** | LoRA å¾®è°ƒä»…éœ€ ~20GBï¼Œ2Ã—V100 è½»æ¾è·‘å…¨å‚ï¼ˆ~50GBï¼‰ |
| **æ”¯æŒ `chat` æ¨¡æ¿** | è‡ªå¸¦å¯¹è¯æ ¼å¼ï¼Œä¸ä½ çš„ `(input, output)` æ•°æ®å¤©ç„¶åŒ¹é… |

> ğŸš« ä¸æ¨è Llama-3-8Bï¼šè™½ç„¶å¼ºå¤§ï¼Œä½†ä¸­æ–‡ä¼˜åŒ–ä¸å¦‚ Qwenï¼Œä¸”éœ€æ›´å¤š token æ‰èƒ½è¡¨è¾¾åŒç­‰è¯­ä¹‰ã€‚  
> ğŸš« ä¸æ¨è ChatGLM3-6Bï¼šç”Ÿæ€å¼±äº Qwenï¼Œå·¥å…·é“¾æ”¯æŒå°‘ã€‚

---

### ğŸ“ æ¨¡å‹ä¿¡æ¯ï¼ˆHugging Faceï¼‰
- **æ¨¡å‹åç§°**ï¼š`Qwen/Qwen1.5-7B-Chat`
- **ä¸‹è½½åœ°å€**ï¼šhttps://huggingface.co/Qwen/Qwen1.5-7B-Chat
- **å‚æ•°é‡**ï¼š7B
- **ä¸Šä¸‹æ–‡é•¿åº¦**ï¼š32768 tokensï¼ˆè¿œè¶…å¾®ä¿¡æ¶ˆæ¯é•¿åº¦ï¼‰

---

### ğŸ›  ä¸‹ä¸€æ­¥è¡ŒåŠ¨æ¸…å•ï¼ˆæŒ‰é¡ºåºæ‰§è¡Œï¼‰

#### 1ï¸âƒ£ åˆ›å»ºé¡¹ç›®ç›®å½•ç»“æ„
```bash
mkdir -p /home/4T/wuhao_zjc/ab_new_document/AIChat_QingZhou/LLaMA-Factory
cd /home/4T/wuhao_zjc/ab_new_document/AIChat_QingZhou/LLaMA-Factory
```

#### 2ï¸âƒ£ å…‹éš† LLaMA-Factory
```bash
git clone https://github.com/hiyouga/LLaMA-Factory.git .
pip install -r requirements.txt
```

#### 3ï¸âƒ£ ä¸‹è½½ Qwen1.5-7B-Chatï¼ˆéœ€å…ˆç™»å½• Hugging Faceï¼‰
```bash
# å¦‚æœæ²¡è£… hf-cliï¼Œå…ˆå®‰è£…
pip install huggingface_hub

# ç™»å½•ï¼ˆè¾“å…¥ä½ çš„ HF Tokenï¼‰
huggingface-cli login

# ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°
huggingface-cli download Qwen/Qwen1.5-7B-Chat \
    --local-dir ./models/Qwen1.5-7B-Chat \
    --local-dir-use-symlinks False
```

> ğŸ’¡ æ¨¡å‹çº¦ 15GBï¼Œç¡®ä¿ `/home/4T` æœ‰è¶³å¤Ÿç©ºé—´ã€‚

#### 4ï¸âƒ£ æ³¨å†Œä½ çš„æ•°æ®é›†
åˆ›å»ºæ–‡ä»¶ï¼š  
`data/wechat_qingzhou.yaml`
```yaml
file_name: /home/4T/wuhao_zjc/ab_new_document/AIChat_QingZhou/Dataset/json/wxid_wg6cp8uh982s22/other_to_you.jsonl
format: alpaca
columns:
  input: input
  output: output
```

> âœ… LLaMA-Factory çš„ `alpaca` æ ¼å¼æ­£å¥½åŒ¹é…ä½ çš„ `{"input": "...", "output": "..."}`ã€‚

#### 5ï¸âƒ£ å‡†å¤‡è®­ç»ƒé…ç½®ï¼ˆLoRAï¼‰
åˆ›å»ºæ–‡ä»¶ï¼š  
`train_qwen_lora.yaml`
```yaml
model_name_or_path: ./models/Qwen1.5-7B-Chat
dataset: wechat_qingzhou
template: qwen
finetuning_type: lora
lora_rank: 64
lora_alpha: 16
lora_dropout: 0.05
output_dir: ./output/qwen1.5-7b-lora-qingzhou
per_device_train_batch_size: 4
gradient_accumulation_steps: 2
lr_scheduler_type: cosine
learning_rate: 3e-4
num_train_epochs: 3
bf16: true
logging_steps: 10
save_steps: 500
ddp_timeout: 1800
```

---

### â–¶ï¸ å¯åŠ¨è®­ç»ƒï¼ˆ2å¡ V100ï¼‰
```bash
cd /home/4T/wuhao_zjc/ab_new_document/AIChat_QingZhou/LLaMA-Factory
torchrun --nproc_per_node=2 \
    src/train.py \
    --config train_qwen_lora.yaml
```

---

### âœ… é¢„æœŸç»“æœ
- è®­ç»ƒå®Œæˆåï¼ŒLoRA æƒé‡ä¿å­˜åœ¨ï¼š  
  `./output/qwen1.5-7b-lora-qingzhou`
- ä½ å¯ä»¥ç”¨å®ƒï¼š
  - åˆå¹¶æˆå®Œæ•´æ¨¡å‹
  - ç›´æ¥ç”¨ LLaMA-Factory æ¨ç†
  - éƒ¨ç½²ä¸º OpenAI å…¼å®¹ API

---

### ğŸ“Œ é‡è¦æé†’
- **ä¸è¦å…¬å¼€åˆ†äº«å¾®è°ƒåçš„æ¨¡å‹**ï¼ˆåŒ…å«ä½ çš„èŠå¤©éšç§ï¼‰
- **è®­ç»ƒæœŸé—´ç›‘æ§æ˜¾å­˜**ï¼š`nvidia-smi`
- **é¦–æ¬¡è®­ç»ƒå»ºè®®å…ˆç”¨å°æ•°æ®æµ‹è¯•**ï¼ˆæ¯”å¦‚å–å‰ 100 è¡Œï¼‰

---

éœ€è¦æˆ‘å¸®ä½ å†™ **åˆå¹¶æ¨¡å‹è„šæœ¬** æˆ– **vLLM éƒ¨ç½²å‘½ä»¤** å—ï¼Ÿè¿˜æ˜¯ç›´æ¥è¿›å…¥è®­ç»ƒï¼Ÿ